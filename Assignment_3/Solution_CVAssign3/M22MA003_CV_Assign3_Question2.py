# -*- coding: utf-8 -*-
"""M22MA003_CV_Assign3_Question2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10b7ujOosCCpCO-HIFpwbzrolu3gXt35D

# New Section
"""

from sklearn.svm import SVC

import cv2
from google.colab.patches import cv2_imshow
import torch
import numpy as np
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import precision_score, recall_score, average_precision_score
import matplotlib.pyplot as plt
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# Step 1: Load and preprocess the CIFAR-10 dataset
from keras.datasets import cifar10

# Load CIFAR-10 dataset
# (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()
(X_train, y_train), (X_test, y_test) = cifar10.load_data()
train_images = X_train[:25000]
train_labels = y_train[:25000]
test_images = X_test[:2500]
test_labels = y_test[:2500]

# Flatten and convert the images to grayscale
train_images_gray = np.reshape(train_images, (train_images.shape[0], -1))
test_images_gray = np.reshape(test_images, (test_images.shape[0], -1))

train_images_gray[14].shape



# function to extract SIFT features from the images
def extract_sift_features(train_images_gray):
    sift=cv2.SIFT_create()
    descriptor_list = []
    for img in train_images_gray:
        img = img.reshape(3, 32, 32).transpose(1, 2, 0)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        kp, des = sift.detectAndCompute(gray, None)
        descriptor_list.append(des)
    for i in range(len(descriptor_list)):
        if descriptor_list[i] is None:
            descriptor_list[i] = np.zeros((1, 128))
    # descriptors = np.concatenate(descriptor_list, axis=0)
    return descriptor_list

def extract_sift_features_test(train_images_gray,kmeans,svm_cl):
    sift=cv2.SIFT_create()
    y_pred=[]
    descriptor_list = []
    for img in train_images_gray:
        img = img.reshape(3, 32, 32).transpose(1, 2, 0)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        kp, des = sift.detectAndCompute(gray, None)
        if des is None:
            des = np.zeros((1, 128))
        
        des = np.pad(des, ((0, 128-len(des)), (0, 0)), 'constant')
        index = kmeans.predict(np.array(des,dtype=np.float64))
        hist, _ = np.histogram(index, bins=100)
        print(hist)
        y_hat = svm_cl.predict([hist])
        y_pred.append(y_hat[0])
    # descriptors = np.concatenate(descriptor_list, axis=0)
    return y_pred

# Draw Precision-Recall Curve
def plot_graph(recall,precision):
    plt.figure(figsize=(10, 8))
    plt.title('Precision-Recall Curve')
    plt.xlabel('Precision')
    plt.ylabel('Recall')
    plt.plot(recall, precision, marker='.', label='Precision-Recall Curve')
    plt.legend()
    plt.show()

# Extract SIFT features from training and testing images
train_features = extract_sift_features(train_images_gray)
descriptors_train = np.concatenate(train_features, axis=0)
test_features = extract_sift_features(test_images_gray)
descriptors_test = np.concatenate(test_features, axis=0)
print(descriptors_train.shape,descriptors_test.shape)

# # sift = cv2.xfeatures2d.SIFT_create(contrastThreshold=0.0)
# sift = cv2.SIFT_create()
# # Function to extract SIFT features from an image
# list1=[]
# def extract_sift_features(image):
#     # for image in train_images:
#     # list1.append(1)
#     img = image.reshape(3, 32, 32).transpose(1, 2, 0)
#     # print(img.shape)
#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
#     list1.append(1)
#     # cv2_imshow(gray)
#     # gray_gpu = torch.cuda.FloatTensor(gray) # move image to GPU
#     # print(type(gray_gpu))
#     # cv2_imshow(gray_gpu)
#     # kps, des = sift.detectAndCompute(gray_gpu.cpu().numpy(), None) # perform SIFT on GPU and move results back to CPU
#     # featvect = des
#     # return featvect
#     kps, des = sift.detectAndCompute(gray, None)
#     return des



# from re import X

# Step 3: Perform k-means clustering to create the visual vocabulary
k = 200 # Number of clusters for k-means
kmeans = KMeans(n_clusters=k, random_state=0,n_init="auto")
kmeans.fit(descriptors_train)
visual_vocabulary = kmeans.cluster_centers_

# Step 4: Compute image-level BoW histograms
def compute_bow_histogram(image_features, visual_vocabulary):
    k = visual_vocabulary.shape[0]
    neigh = NearestNeighbors(n_neighbors=1)
    neigh.fit(visual_vocabulary)
    nearest_centroids = neigh.kneighbors(image_features, return_distance=False)
    histogram, _ = np.histogram(nearest_centroids, bins=range(k+1))
    return histogram

# create histograms for each image
def compute_bow_histogram(image_features):
    histograms = []
    for des in image_features:
        histogram = np.zeros(200)
        for i in range(len(des)):
            index = kmeans.predict([des[i]])
            histogram[index] += 1
        histograms.append(histogram)
    return histograms


# Compute BoW histograms for training and testing images
train_histograms = np.array(compute_bow_histogram(train_features))
test_histograms = np.array(compute_bow_histogram(test_features))
x = np.array(train_histograms)
y = np.array(train_labels)
svm_cl = SVC(kernel='linear', C=1.0).fit(x, y)

random_index = np.random.randint(0, len(test_images_gray))
# random_index = 
test_img,test_img_label = test_images_gray[random_index], test_labels[random_index]
print(test_img.shape, test_img_label)

# detect SIFT features
sift=cv2.SIFT_create()
kp, des = sift.detectAndCompute(test_img.reshape(3, 32, 32).transpose(1, 2, 0), None)
print(des)
index = kmeans.predict(np.array(des,dtype=np.float64))

# make histogram for the query image
query_histogram, _ = np.histogram(index, bins=100, range=(0, 100))

# predict the class of the query image
y_pred = svm_cl.predict([query_histogram])
class_to_label = {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}
print(f"Label Predicted: {class_to_label[y_pred[0]]}")
print(f"Ground Truth Label: {class_to_label[test_img_label[0]]}")

# retrieve the images from the training data that are similar to the query image
idx = np.where(train_labels == y_pred[0])[0]
sim_images=idx[0:5]
# for i in range(len(sim_images)):
#   print(sim_images[i])
#   cv2_imshow(train_images[i])

import matplotlib.pyplot as plt

# Set up the figure with subplots
fig, axes = plt.subplots(2, 3, figsize=(10, 6))
axes = axes.flatten()

# Plot the query image
axes[0].imshow(test_images[random_index])
axes[0].set_title("Query Image")
axes[0].axis('off')

# Plot the top 5 similar images
for i in range(5):
    axes[i+1].imshow(train_images[sim_images[i]])
    axes[i+1].set_title(f"Similar Image {i+1}")
    axes[i+1].axis('off')

# Show the plot
plt.tight_layout()
plt.show()

# perform the same steps for the test data
predicted_labels = []
for i in range(len(test_images_gray)):
    data = np.array(test_images_gray[i]).reshape(3, 32, 32).transpose(1, 2, 0)
    kp, des = sift.detectAndCompute(data, None)
    if des is None:
        des = np.zeros((1, 128))
    des = np.pad(des, ((0, 128-len(des)), (0, 0)), 'constant')
    pos = kmeans.predict(np.array(des,dtype=np.float64))
    words, _ = np.histogram(pos, bins=200)
    high_freq_bins = svm_cl.predict([words])
    predicted_labels.append(high_freq_bins[0])
y_pred=predicted_labels

# calculate the accuracy, precision, recall and f1-score, AP and mAP
p_value = np.sum(np.array(y_pred) == test_labels) / np.sum(np.array(y_pred) == 2)/len(test_labels)
recall_value = np.sum(np.array(y_pred) == test_labels) / np.sum(test_labels == 2)/len(test_labels)
acc = np.sum(np.array(y_pred) == test_labels) / len(y)
AP = np.sum(np.array(y_pred) == test_labels) / len(y)
f1_score = 2 * (p_value * recall_value) / (p_value + recall_value)

print(f"Accuracy is {acc:3f} | Precision is {recall_value:3f}")
print(f"Recall is {recall_value:3f} | F1_score is {f1_score*100:3f} % | AP is {AP:3f}")

import numpy as np

def precision_recall_curve(test_labels, predicted_y):
    true_positive=0
    false_postive = 0
    order = np.argsort(predicted_y)[::-1]
    test_labels = np.asarray(test_labels)[order]
    sigma = np.zeros_like(test_labels, dtype=float)
    predicted_y = np.asarray(predicted_y)[order]
    recal_value = np.zeros_like(test_labels, dtype=float)
    false_negative = np.sum(test_labels)
    p_value = np.zeros_like(test_labels, dtype=float)
    for i in range(len(predicted_y)):
        if test_labels[i]:
            false_negative -= 1
            true_positive += 1
        else:
            false_postive += 1
        sigma[i] = predicted_y[i]
        p_value[i] = true_positive / (true_positive + false_postive)
        recal_value[i] = true_positive / (true_positive + false_negative)
    return p_value, recal_value, sigma

p_value, recal_value, _ = precision_recall_curve(test_labels, y_pred)

plot_graph(recal_value, p_value)

